# -*- coding: utf-8 -*-
"""datamodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e7Eas4nZJIig4uxUhGSOzfnj2KVhcAwa
"""

#datacleaning
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk import WordNetLemmatizer
from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import string
import xgboost as xgb
depression_data=pd.read_csv(r"depression_dataset_reddit_cleaned.csv")
anxiety_data=pd.read_excel(r"anxietydataset.xlsx")
stress_data=pd.read_csv(r"stressdataset.csv")
anxiety_data=anxiety_data.dropna(how='any')
stress_data=stress_data.dropna(how='any')
anxiety_data['text']=anxiety_data['text'].apply(lambda post:re.sub(r"\'"," ",post))
depress_train=depression_data.sample(frac=0.8,random_state=50)
depress_test=depression_data.drop(depress_train.index)
stress_data['text']=stress_data['text'].apply(lambda post:re.sub("\'"," ",post))
contractions_dict = { "ain t ": "are not "," s ":" is ","aren t ": "are not ","can t ": "can not ","can t ve ": "cannot have ",
" cause ": "because ","could ve ": "could have ","couldn t ": "could not ","couldn t ve ": "could not have ",
"didn t ": "did not ","doesn t ": "does not ","don t ": "do not ","hadn t ": "had not ","hadn t ve ": "had not have ",
"hasn t ": "has not ","haven t ": "have not ","he d ": "he would ","he d ve ": "he would have ","he ll ": "he will ",
"he ll ve ": "he will have ","how d ": "how did ","how d y ": "how do you ","how ll ": "how will ","i d ": "i would ",
"i d ve ": "i would have ","i ll ": "i will ","i ll ve ": "i will have ","i m ": "i am ","i ve ": "i have ",
"isn t ": "is not ","it d ": "it would ","it d ve ": "it would have ","it ll ": "it will ","it ll ve ": "it will have ",
"let s ": "let us ","ma am ": "madam ","mayn t ": "may not ","might ve ": "might have ","mightn t ": "might not ",
"mightn t ve ": "might not have ","must ve ": "must have ","mustn t ": "must not ","mustn t ve ": "must not have ",
"needn t ": "need not ","needn t ve ": "need not have ","o clock ": "of the clock ","oughtn t ": "ought not ",
"oughtn t ve ": "ought not have ","shan t ": "shall not ","sha n t ": "shall not ",
"shan t ve ": "shall not have ","she d ": "she would ","she d ve ": "she would have ","she ll ": "she will ",
"she ll ve ": "she will have ","should ve ": "should have ","shouldn t ": "should not ",
"shouldn t ve ": "should not have ","so ve ": "so have ","that d ": "that would ","that d ve ": "that would have ",
"there d ": "there would ","there d ve ": "there would have ",
"they d ": "they would ","they d ve ": "they would have ","they ll ": "they will ","they ll ve ": "they will have ",
"they re ": "they are ","they ve ": "they have ","to ve ": "to have ","wasn t ": "was not ","we d ": "we would ",
"we d ve ": "we would have ","we ll ": "we will ","we ll ve ": "we will have ","we re ": "we are ","we ve ": "we have ",
"weren t ": "were not ","what ll ": "what will ","what ll ve ": "what will have ","what re ": "what are ",
"what ve ": "what have ","when ve ": "when have ","where d ": "where did ",
"where ve ": "where have ","who ll ": "who will ","who ll ve ": "who will have ","who ve ": "who have ",
"why ve ": "why have ","will ve ": "will have ","won t ": "will not ","won t ve ": "will not have ",
"would ve ": "would have ","wouldn t ": "would not ","wouldn t ve ": "would not have ","y all ": "you all ",
"y all d ": "you all would ","y all d ve ": "you all would have ","y all re ": "you all are ","y all ve ": "you all have ",
"you d ": "you would ","you d ve ": "you would have ","you ll ": "you will ","you ll ve ": "you will have ",
"you re ": "you are ","you ve ": "you have ", "aint ": "are not ","arent ": "are not ","cant ": "can not ","cant ve ": "cannot have ",
"couldve ": "could have ","couldnt ": "could not ","couldnt ve ": "could not have ",
"didnt ": "did not ","doesnt ": "does not ","dont ": "do not ","hadnt ": "had not ","hadnt ve ": "had not have ",
"hasnt ": "has not ","havent ": "have not ","hed ": "he would ","hed ve ": "he would have ",
"howd ": "how did ","howd y ": "how do you ","howll ": "how will ","id ": "i would ",
"id ve ": "i would have ","ill ve ": "i will have ","im ": "i am ","ive ": "i have ",
"isnt ": "is not ","itd ": "it would ","itd ve ": "it would have ","itll ": "it will ","itll ve ": "it will have ",
"lets ": "let us ","maam ": "madam ","mightve ": "might have ","mightnt ": "might not ",
"mightnt ve ": "might not have ","mustve ": "must have ","mustnt ": "must not ","mustnt ve ": "must not have ",
"neednt ": "need not ","neednt ve ": "need not have ","oclock ": "of the clock ","oughtnt ": "ought not ",
"oughtnt ve ": "ought not have ","shant ": "shall not ",
"shant ve ": "shall not have ","shed ": "she would ","shed ve ": "she would have ","shell ": "she will ",
"shell ve ": "she will have ","shouldve ": "should have ","shouldnt ": "should not ",
"shouldnt ve ": "should not have ","sove ": "so have ","thatd ": "that would ","thatd ve ": "that would have ",
"thered ": "there would ","thered ve ": "there would have ",
"theyd ": "they would ","theyd ve ": "they would have ","theyll ": "they will ","theyll ve ": "they will have ",
"theyre ": "they are ","theyve ": "they have ","tove ": "to have ","wasnt ": "was not ","wed ": "we would ",
"weve ": "we have ",
"werent ": "were not ","whatll ": "what will ","whatre ": "what are ",
"whatve ": "what have ","whenve ": "when have ","whered ": "where did ",
"whereve ": "where have ","wholl ": "who will ","whove ": "who have ",
"whyve ": "why have ","willve ": "will have ","wont ": "will not ",
"wouldnt ": "would not ","yall ": "you all ",
"yall d ": "you all would ","yall d ve ": "you all would have ","yall re ": "you all are ","yall ve ": "you all have ",
"youd ": "you would ","youd ve ": "you would have ","youll ": "you will ","youll ve ": "you will have ",
"youre ": "you are ","youve ": "you have "}
def contractions(post):  
  for key,value in contractions_dict.items():
    if key in post:
      post=re.sub(key,value,post)
  return post
def cleaning(post):
  post=post.lower()
  post=re.sub(r"http\S+","",post)
  post=re.sub(r"url"," ",post)
  post=re.sub(r"\s[^a-z]\s"," ",post)
  post=re.sub(r"\sop\s"," ",post)
  post=re.sub(r"\s[a-z]\s"," ",post)  
  post=re.sub(r"\s[a-z][a-z]\s"," ",post)
  post=re.sub(r"\w*\d\w*","",post)
  post=re.sub(r"\."," ",post)
  post=re.sub(r"\""," ",post)
  post=re.sub(r"\,"," ",post)
  post=re.sub(r"\(|\)|\[|\]|\{|\}"," ",post)
  post=re.sub(r"\+|\-|\/|\=|\*"," ",post)
  post.translate(str.maketrans('','',string.punctuation))
  post=re.sub(r"\?"," ",post)
  post=re.sub(r"\!"," ",post)
  post=re.sub(r"[^a-z]"," ",post)

  return post
depress_train["clean_text"]=depress_train["clean_text"].apply(lambda post:contractions(post))
depress_test["clean_text"]=depress_test["clean_text"].apply(lambda post:contractions(post))
depress_train["clean_text"]=depress_train["clean_text"].apply(lambda post:cleaning(post))
depress_test["clean_text"]=depress_test["clean_text"].apply(lambda post:cleaning(post))
depress_train["clean_text"]=depress_train["clean_text"].apply(lambda post:re.sub(" +"," ",post))
depress_test["clean_text"]=depress_test["clean_text"].apply(lambda post:re.sub(" +"," ",post))
anxiety_data["text"]=anxiety_data["text"].apply(lambda post:contractions(post))
anxiety_data["text"]=anxiety_data["text"].apply(lambda post:cleaning(post))
anxiety_data["text"]=anxiety_data["text"].apply(lambda post:re.sub(" +"," ",post))
stress_data["text"]=stress_data["text"].apply(lambda post:contractions(post))
stress_data["text"]=stress_data["text"].apply(lambda post:cleaning(post))
stress_data["text"]=stress_data["text"].apply(lambda post:re.sub(" +"," ",post))
anxiety_data.dropna(inplace=True)
depress_train.dropna(inplace=True)
depress_test.dropna(inplace=True)
stress_data.dropna(inplace=True)
stop_words=set(stopwords.words("english"))
depress_train["clean_text"]=[word_tokenize(w) for w in depress_train["clean_text"]]
depress_test["clean_text"]=[word_tokenize(w) for w in depress_test["clean_text"]]
anxiety_data["text"]=[word_tokenize(w) for w in anxiety_data["text"]]
stress_data["text"]=[word_tokenize(w) for w in stress_data["text"]]
cleaned_words=[] 
#removing stopwords
for words in depress_train["clean_text"]:
  temp=[]
  for w in words:
    if w not in stop_words:
      temp.append(w)
  cleaned_words.append(temp)
depress_train["clean_text"]=cleaned_words
cleaned_words=[]
for words in depress_test["clean_text"]:
  temp=[]
  for w in words:
    if w not in stop_words:
      temp.append(w)
  cleaned_words.append(temp)
depress_test["clean_text"]=cleaned_words
cleaned_words=[]
for words in anxiety_data["text"]:
  temp=[]
  for w in words:
    if w not in stop_words:
      temp.append(w)
  cleaned_words.append(temp)
anxiety_data["text"]=cleaned_words
cleaned_words=[]
for words in stress_data["text"]:
  temp=[]
  for w in words:
    if w not in stop_words:
      temp.append(w)
  cleaned_words.append(temp)
stress_data["text"]=cleaned_words      
#lemmatzing by parts of speechh
pos=[]
for words in depress_train["clean_text"]:
  temp=nltk.pos_tag(words)
  pos.append(temp)
allowed_type=["JJ","RB","JJR","JJS","RBR","RBS","NN","NNS","NNP"]
convert={"a":["JJ","JJR","JJS"],"r":["RB","RBR","RBS"],"n":["NN","NNS","NNP"]}
cleaned_words=[]
lematizer=WordNetLemmatizer();
for words in pos:
  temp=[]
  for w in words:
    if w[1] in allowed_type:
      for key,val in convert.items():
        if w[1] in val:
          temp.append(lematizer.lemmatize(w[0],pos=key)) 
  cleaned_words.append(temp)
depress_train["clean_text"]=cleaned_words  

pos=[]
for words in depress_test["clean_text"]:
  temp=nltk.pos_tag(words)
  pos.append(temp)
cleaned_words=[]
lematizer=WordNetLemmatizer();
for words in pos:
  temp=[]
  for w in words:
    if w[1] in allowed_type:
      for key,val in convert.items():
        if w[1] in val:
          temp.append(lematizer.lemmatize(w[0],pos=key)) 
  cleaned_words.append(temp)
depress_test["clean_text"]=cleaned_words  
pos=[]
for words in anxiety_data["text"]:
  temp=nltk.pos_tag(words)
  pos.append(temp)
cleaned_words=[]
lematizer=WordNetLemmatizer();
for words in pos:
  temp=[]
  for w in words:
    if w[1] in allowed_type:
      for key,val in convert.items():
        if w[1] in val:
          temp.append(lematizer.lemmatize(w[0],pos=key)) 
  cleaned_words.append(temp)
anxiety_data["text"]=cleaned_words
pos=[]
for words in stress_data["text"]:
  temp=nltk.pos_tag(words)
  pos.append(temp)
cleaned_words=[]
lematizer=WordNetLemmatizer();
for words in pos:
  temp=[]
  for w in words:
    if w[1] in allowed_type:
      for key,val in convert.items():
        if w[1] in val:
          temp.append(lematizer.lemmatize(w[0],pos=key)) 
  cleaned_words.append(temp)
stress_data["text"]=cleaned_words 
depress_train["clean_text"]=[list(set(t)) for t in depress_train["clean_text"]]
depress_test["clean_text"]=[list(set(t)) for t in depress_test["clean_text"]]
stress_data["text"]=[list(set(t)) for t in stress_data["text"]]
anxiety_data["text"]=[list(set(t)) for t in anxiety_data["text"]]
anxiety_train=anxiety_data.sample(frac=0.8,random_state=50)
anxiety_test=anxiety_data.drop(anxiety_train.index)
stress_train=stress_data.sample(frac=0.8,random_state=50)
stress_test=stress_data.drop(stress_train.index)

dprs_word2vecmodel=Word2Vec(depress_train["clean_text"].to_list(),vector_size=100,window=3,min_count=2, workers=4,sg=1)
anxty_word2vecmodel=Word2Vec(anxiety_train["text"].to_list(),vector_size=100,window=3,min_count=2, workers=4,sg=1)
strs_word2vecmodel=Word2Vec(stress_train["text"].to_list(),vector_size=100,window=3,min_count=2, workers=4,sg=1)

import numpy as np
#vectorization
def dprs_word2vectortest(tokenizedpost):
  vectors=[]
  if len(tokenizedpost)<1:
    return np.zeros(100)
  else:
    for word in tokenizedpost:
      if word in dprs_word2vecmodel.wv:
        vectors.append(dprs_word2vecmodel.wv.get_vector(word))
      else:
        vectors.append(np.random.rand(100))
  return np.mean(vectors,axis=0)          
def anx_word2vectortest(tokenizedpost):
  vectors=[]
  if len(tokenizedpost)<1:
    return np.zeros(100)
  else:
    for word in tokenizedpost:
      if word in anxty_word2vecmodel.wv:
        vectors.append(anxty_word2vecmodel.wv.get_vector(word))
      else:
        vectors.append(np.random.rand(100))
  return np.mean(vectors,axis=0)
def strs_word2vectortest(tokenizedpost):
  vectors=[]
  if len(tokenizedpost)<1:
    return np.zeros(100)
  else:
    for word in tokenizedpost:
      if word in strs_word2vecmodel.wv:
        vectors.append(strs_word2vecmodel.wv.get_vector(word))
      else:
        vectors.append(np.random.rand(100))
  return np.mean(vectors,axis=0)      
depress_test.dropna(inplace=True)
anxiety_test.dropna(inplace=True)
depress_train.dropna(inplace=True)
anxiety_train.dropna(inplace=True)
stress_train.dropna(inplace=True)
stress_test.dropna(inplace=True)
depress_test=depress_test[depress_test.astype(str)['clean_text'] != '[]']
anxiety_test=anxiety_test[anxiety_test.astype(str)['text'] != '[]']
depress_train=depress_train[depress_train.astype(str)['clean_text'] != '[]']
anxiety_train=anxiety_train[anxiety_train.astype(str)['text'] != '[]']
stress_train=stress_train[stress_train.astype(str)['text'] != '[]']
stress_test=stress_test[stress_test.astype(str)['text'] != '[]']
depress_test["vectors"]=depress_test["clean_text"].apply(lambda post:dprs_word2vectortest(post))
anxiety_test["vectors"]=anxiety_test["text"].apply(lambda post:anx_word2vectortest(post))
stress_test["vectors"]=stress_test["text"].apply(lambda post:strs_word2vectortest(post))
depress_train["vectors"]=depress_train["clean_text"].apply(lambda post:dprs_word2vectortest(post))
anxiety_train["vectors"]=anxiety_train["text"].apply(lambda post:anx_word2vectortest(post))
stress_train["vectors"]=stress_train["text"].apply(lambda post:strs_word2vectortest(post))
from numpy.linalg import norm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score      
dep_rf=xgb.XGBClassifier(objective="binary:logistic", random_state=42)
y=np.array([np.array(post) for post in depress_train["vectors"]])
# print(y.shape)
deprf_model=dep_rf.fit(y,depress_train["is_depression"].values.ravel())
# y=np.array([np.array(post) for post in depress_test["vectors"]])
# dep_pred=deprf_model.predict(y)
# print(accuracy_score(depress_test["is_depression"],dep_pred))
def depress_rf(vector):
  depress_pred=deprf_model.predict(vector)
  return depress_pred
anx_rf=xgb.XGBClassifier(objective="binary:logistic", random_state=42)
y=[np.array(post) for post in anxiety_train["vectors"]]
anxrf_model=anx_rf.fit(y,anxiety_train["label"])
# y=np.array([np.array(post) for post in anxiety_test["vectors"]])
# anx_pred=anxrf_model.predict(y)
# print(accuracy_score(anxiety_test["label"],anx_pred))
def anxiety_rf(vector):
  anxiety_pred=anxrf_model.predict(vector)
  return anxiety_pred
strs_rf=xgb.XGBClassifier(objective="binary:logistic", random_state=42)
y=[np.array(post) for post in stress_train["vectors"]]
strsrf_model=strs_rf.fit(y,stress_train["label"])
# y=np.array([np.array(post) for post in stress_test["vectors"]])
# strs_pred=strsrf_model.predict(y)
# print(accuracy_score(stress_test["label"],strs_pred))
def stress_rf(vector):
  stress_pred=strsrf_model.predict(vector)
  return stress_pred
def depressedsimilar(vector):
  df1=depress_test[depress_test["is_depression"]==1]
  df2=depress_train[depress_train["is_depression"]==1]
  depressed=pd.concat([df1,df2])
  test=np.array(vector)
  templist=[]
  for postvector in depressed["vectors"]:
    pvarray=np.array(postvector)
    templist.append(np.dot(test,pvarray)/(norm(test)*norm(pvarray)))
  depressed['similarity']=templist
  depressed.sort_values(by="similarity",ascending=False,inplace=True)
  mostdepressed=depressed["similarity"].iloc[0]
  return mostdepressed
def anxioussimilar(vector):
  test=np.array(vector)
  df1=anxiety_test[anxiety_test["label"]==1]
  df2=anxiety_train[anxiety_train["label"]==1]
  anxious=pd.concat([df1,df2])
  templist=[]
  for postvector in anxious["vectors"]:
    pvarray=np.array(postvector)
    templist.append(np.dot(test,pvarray)/(norm(test)*norm(pvarray)))
  anxious['similarity']=templist
  anxious.sort_values(by="similarity",ascending=False,inplace=True)
  mostanxious=anxious["similarity"].iloc[0]
  return mostanxious
def stresssimilar(vector):
  df1=stress_test[stress_test["label"]==1]
  df2=stress_train[stress_train["label"]==1]
  stressed=pd.concat([df1,df2])
  test=np.array(vector)
  templist=[]
  for postvector in stressed["vectors"]:
    pvarray=np.array(postvector)
    templist.append(np.dot(test,pvarray)/(norm(test)*norm(pvarray)))
  stressed['similarity']=templist
  stressed.sort_values(by="similarity",ascending=False,inplace=True)
  moststressed=stressed["similarity"].iloc[0]
  return moststressed
def DASlabel(query):

  text=re.sub("\'"," ",query)
  text=contractions(text)
  text=cleaning(text)
  post=re.sub(" +"," ",text)
  post=word_tokenize(post)
  stop_words=set(stopwords.words("english"))
  temp=[]
  for w in post:
    if w not in stop_words:
      temp.append(w)
  post=temp
  pos=[]
  pos=nltk.pos_tag(post)
  allowed_type=["JJ","RB","JJR","JJS","RBR","RBS","NN","NNS","NNP"]
  convert={"a":["JJ","JJR","JJS"],"r":["RB","RBR","RBS"],"n":["NN","NNS","NNP"]}
  cleaned_words=[]
  lematizer=WordNetLemmatizer();
  for w in pos:
    if w[1] in allowed_type:
      for key,val in convert.items():
        if w[1] in val:
          cleaned_words.append(lematizer.lemmatize(w[0],pos=key)) 
  post=list(set(cleaned_words))
  dvectorizedpost=dprs_word2vectortest(post)
  avectorizedpost=anx_word2vectortest(post)
  svectorizedpost=strs_word2vectortest(post)
  dv=np.array([np.array(dvectorizedpost)])
  av=np.array([np.array(avectorizedpost)])
  sv=np.array([np.array(svectorizedpost)])
  depressed,anxious,stressed=depress_rf(dv),anxiety_rf(av),stress_rf(sv)
  label=" "
  if(int(depressed)>int(anxious) and int(depressed)>int(stressed)):
    label="D"
  elif(int(anxious)>int(depressed) and int(anxious)>int(stressed)):
    label="A"
  elif(int(stressed)>int(depressed) and int(stressed)>int(anxious)):
    label="S"
  elif(int(anxious)==int(depressed) and int(depressed)==1 and int(stressed)==0):
    dv=depressedsimilar(dvectorizedpost)
    av=anxioussimilar(avectorizedpost)
    if(dv.all()>av.all()):
      label="D"
    else:
      label="A"
  elif(int(anxious)==int(stressed) and int(stressed)==1 and int(depressed)==0):
    sv=stresssimilar(svectorizedpost)
    av=anxioussimilar(avectorizedpost)
    if(sv.all()>av.all()):
      label="S"
    else:
      label="A"    
  elif(int(stressed)==int(depressed) and int(depressed)==1 and int(anxious)==0):
    dv=depressedsimilar(dvectorizedpost)
    sv=stresssimilar(svectorizedpost)
    if(dv.all()>sv.all()):
      label="D"
    else:
      label="S"
  else:
    dv=depressedsimilar(dvectorizedpost)
    sv=stresssimilar(svectorizedpost)
    av=anxioussimilar(avectorizedpost)
    if(dv.all()>sv.all() and dv.all()>av.all()):
      label="D"
    elif(sv.all()>dv.all() and sv.all()>av.all()):
      label="S"
    else:
      label="A"
  return label   
# import whisper

# def videototext(videopath):
#   model1 = whisper.load_model("base")
#   text = model1.transcribe(videopath,fp16=False,language="en")
#   return text['text']

# #precisioncalculation
# def meanprecision(vector):
#   result=pd.DataFrame()
#   test=np.array(vector)
#   temp=[]
#   result['text']=depress_test["clean_text"]
#   result['vector']=depress_test["vectors"]
#   result['label']=depress_test["is_depression"]
#   for postvector in result["vector"]:
#     pvarray=np.array(postvector)
#     if (norm(test)*norm(pvarray))==0:
#       exit()
#     else:
#       temp.append(np.dot(test,pvarray)/(norm(test)*norm(pvarray)))
#   result['similarity']=temp
#   result.sort_values(by="similarity",ascending=False,inplace=True)
#   ranking=list(result["label"].head(10))
#   precision=[]
#   for i in range(1,11):
#     precision.append(np.sum(ranking[:i])/i)
#   if precision==[]:
#     return 0
#   return np.mean(precision)
# precision=[]
# for i,post in depress_test.iterrows():
#   if post["is_depression"]==0:
#     temp=meanprecision(post["vectors"])
#     precision.append(1-temp)
#   else:
#     precision.append(meanprecision(post["vectors"]))  





# # print("precision:",np.mean(precision))




