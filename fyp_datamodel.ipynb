{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLiUmpXltVoM",
        "outputId": "ec90a377-09df-4935-83d2-b00099284a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQDWvJzi3tH9",
        "outputId": "61895f56-efb0-4f82-cff7-0b3285535a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6084 sha256=ca09b0221720153913f6fd30facdd5a1b82b590f16751778560bfdc1e49ef64f\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/33/46/5ab7eca55b9490dddbf3441c68a29535996270ef1ce8b9b6d7\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-0fe1rmsy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-0fe1rmsy\n",
            "  Resolved https://github.com/openai/whisper.git to commit 7858aa9c08d98f75575035ecd6481f462d66ca27\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.16.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->openai-whisper==20230124) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (1.26.14)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230124-py3-none-any.whl size=1179424 sha256=d70143579facee96ab119a434c3b8bc3894f653833cffe57480eb5c64eca1010\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cn1fj56d/wheels/a7/70/18/b7693c07b1d18b3dafb328f5d0496aa0d41a9c09ef332fd8e6\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tokenizers, ffmpeg-python, huggingface-hub, transformers, openai-whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.12.1 openai-whisper-20230124 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19792 sha256=f94ae38d230f62d5bf1d0d664f00537c293a92d4458a36623ad08e6ea81ce3c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/f2/70/526da675d32f17577ec47ac4c663084efe39d47c826b6c3bb1\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40u-15EXsHz7",
        "outputId": "0eda7cde-1371-4d40-d929-9f206b67416b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ba9bedb890f4>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  anxiety_data['text']=anxiety_data['text'].apply(lambda post:re.sub(r\"\\'\",\" \",post))\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"datamodel.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1e7Eas4nZJIig4uxUhGSOzfnj2KVhcAwa\n",
        "\"\"\"\n",
        "\n",
        "#datacleaning\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import string\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "depression_data=pd.read_csv(r\"depression_dataset_reddit_cleaned.csv\")\n",
        "anxiety_data=pd.read_excel(r\"anxietydataset.xlsx\")\n",
        "stress_data=pd.read_csv(r\"stressdataset.csv\")\n",
        "anxiety_data=anxiety_data.dropna(how='any')\n",
        "stress_data=stress_data.dropna(how='any')\n",
        "anxiety_data['text']=anxiety_data['text'].apply(lambda post:re.sub(r\"\\'\",\" \",post))\n",
        "depress_train=depression_data.sample(frac=0.8,random_state=50)\n",
        "depress_test=depression_data.drop(depress_train.index)\n",
        "stress_data['text']=stress_data['text'].apply(lambda post:re.sub(\"\\'\",\" \",post))\n",
        "contractions_dict = { \"ain t \": \"are not \",\" s \":\" is \",\"aren t \": \"are not \",\"can t \": \"can not \",\"can t ve \": \"cannot have \",\n",
        "\" cause \": \"because \",\"could ve \": \"could have \",\"couldn t \": \"could not \",\"couldn t ve \": \"could not have \",\n",
        "\"didn t \": \"did not \",\"doesn t \": \"does not \",\"don t \": \"do not \",\"hadn t \": \"had not \",\"hadn t ve \": \"had not have \",\n",
        "\"hasn t \": \"has not \",\"haven t \": \"have not \",\"he d \": \"he would \",\"he d ve \": \"he would have \",\"he ll \": \"he will \",\n",
        "\"he ll ve \": \"he will have \",\"how d \": \"how did \",\"how d y \": \"how do you \",\"how ll \": \"how will \",\"i d \": \"i would \",\n",
        "\"i d ve \": \"i would have \",\"i ll \": \"i will \",\"i ll ve \": \"i will have \",\"i m \": \"i am \",\"i ve \": \"i have \",\n",
        "\"isn t \": \"is not \",\"it d \": \"it would \",\"it d ve \": \"it would have \",\"it ll \": \"it will \",\"it ll ve \": \"it will have \",\n",
        "\"let s \": \"let us \",\"ma am \": \"madam \",\"mayn t \": \"may not \",\"might ve \": \"might have \",\"mightn t \": \"might not \",\n",
        "\"mightn t ve \": \"might not have \",\"must ve \": \"must have \",\"mustn t \": \"must not \",\"mustn t ve \": \"must not have \",\n",
        "\"needn t \": \"need not \",\"needn t ve \": \"need not have \",\"o clock \": \"of the clock \",\"oughtn t \": \"ought not \",\n",
        "\"oughtn t ve \": \"ought not have \",\"shan t \": \"shall not \",\"sha n t \": \"shall not \",\n",
        "\"shan t ve \": \"shall not have \",\"she d \": \"she would \",\"she d ve \": \"she would have \",\"she ll \": \"she will \",\n",
        "\"she ll ve \": \"she will have \",\"should ve \": \"should have \",\"shouldn t \": \"should not \",\n",
        "\"shouldn t ve \": \"should not have \",\"so ve \": \"so have \",\"that d \": \"that would \",\"that d ve \": \"that would have \",\n",
        "\"there d \": \"there would \",\"there d ve \": \"there would have \",\n",
        "\"they d \": \"they would \",\"they d ve \": \"they would have \",\"they ll \": \"they will \",\"they ll ve \": \"they will have \",\n",
        "\"they re \": \"they are \",\"they ve \": \"they have \",\"to ve \": \"to have \",\"wasn t \": \"was not \",\"we d \": \"we would \",\n",
        "\"we d ve \": \"we would have \",\"we ll \": \"we will \",\"we ll ve \": \"we will have \",\"we re \": \"we are \",\"we ve \": \"we have \",\n",
        "\"weren t \": \"were not \",\"what ll \": \"what will \",\"what ll ve \": \"what will have \",\"what re \": \"what are \",\n",
        "\"what ve \": \"what have \",\"when ve \": \"when have \",\"where d \": \"where did \",\n",
        "\"where ve \": \"where have \",\"who ll \": \"who will \",\"who ll ve \": \"who will have \",\"who ve \": \"who have \",\n",
        "\"why ve \": \"why have \",\"will ve \": \"will have \",\"won t \": \"will not \",\"won t ve \": \"will not have \",\n",
        "\"would ve \": \"would have \",\"wouldn t \": \"would not \",\"wouldn t ve \": \"would not have \",\"y all \": \"you all \",\n",
        "\"y all d \": \"you all would \",\"y all d ve \": \"you all would have \",\"y all re \": \"you all are \",\"y all ve \": \"you all have \",\n",
        "\"you d \": \"you would \",\"you d ve \": \"you would have \",\"you ll \": \"you will \",\"you ll ve \": \"you will have \",\n",
        "\"you re \": \"you are \",\"you ve \": \"you have \", \"aint \": \"are not \",\"arent \": \"are not \",\"cant \": \"can not \",\"cant ve \": \"cannot have \",\n",
        "\"couldve \": \"could have \",\"couldnt \": \"could not \",\"couldnt ve \": \"could not have \",\n",
        "\"didnt \": \"did not \",\"doesnt \": \"does not \",\"dont \": \"do not \",\"hadnt \": \"had not \",\"hadnt ve \": \"had not have \",\n",
        "\"hasnt \": \"has not \",\"havent \": \"have not \",\"hed \": \"he would \",\"hed ve \": \"he would have \",\n",
        "\"howd \": \"how did \",\"howd y \": \"how do you \",\"howll \": \"how will \",\"id \": \"i would \",\n",
        "\"id ve \": \"i would have \",\"ill ve \": \"i will have \",\"im \": \"i am \",\"ive \": \"i have \",\n",
        "\"isnt \": \"is not \",\"itd \": \"it would \",\"itd ve \": \"it would have \",\"itll \": \"it will \",\"itll ve \": \"it will have \",\n",
        "\"lets \": \"let us \",\"maam \": \"madam \",\"mightve \": \"might have \",\"mightnt \": \"might not \",\n",
        "\"mightnt ve \": \"might not have \",\"mustve \": \"must have \",\"mustnt \": \"must not \",\"mustnt ve \": \"must not have \",\n",
        "\"neednt \": \"need not \",\"neednt ve \": \"need not have \",\"oclock \": \"of the clock \",\"oughtnt \": \"ought not \",\n",
        "\"oughtnt ve \": \"ought not have \",\"shant \": \"shall not \",\n",
        "\"shant ve \": \"shall not have \",\"shed \": \"she would \",\"shed ve \": \"she would have \",\"shell \": \"she will \",\n",
        "\"shell ve \": \"she will have \",\"shouldve \": \"should have \",\"shouldnt \": \"should not \",\n",
        "\"shouldnt ve \": \"should not have \",\"sove \": \"so have \",\"thatd \": \"that would \",\"thatd ve \": \"that would have \",\n",
        "\"thered \": \"there would \",\"thered ve \": \"there would have \",\n",
        "\"theyd \": \"they would \",\"theyd ve \": \"they would have \",\"theyll \": \"they will \",\"theyll ve \": \"they will have \",\n",
        "\"theyre \": \"they are \",\"theyve \": \"they have \",\"tove \": \"to have \",\"wasnt \": \"was not \",\"wed \": \"we would \",\n",
        "\"weve \": \"we have \",\n",
        "\"werent \": \"were not \",\"whatll \": \"what will \",\"whatre \": \"what are \",\n",
        "\"whatve \": \"what have \",\"whenve \": \"when have \",\"whered \": \"where did \",\n",
        "\"whereve \": \"where have \",\"wholl \": \"who will \",\"whove \": \"who have \",\n",
        "\"whyve \": \"why have \",\"willve \": \"will have \",\"wont \": \"will not \",\n",
        "\"wouldnt \": \"would not \",\"yall \": \"you all \",\n",
        "\"yall d \": \"you all would \",\"yall d ve \": \"you all would have \",\"yall re \": \"you all are \",\"yall ve \": \"you all have \",\n",
        "\"youd \": \"you would \",\"youd ve \": \"you would have \",\"youll \": \"you will \",\"youll ve \": \"you will have \",\n",
        "\"youre \": \"you are \",\"youve \": \"you have \"}\n",
        "def contractions(post):  \n",
        "  for key,value in contractions_dict.items():\n",
        "    if key in post:\n",
        "      post=re.sub(key,value,post)\n",
        "  return post\n",
        "def cleaning(post):\n",
        "  post=post.lower()\n",
        "  post=re.sub(r\"http\\S+\",\"\",post)\n",
        "  post=re.sub(r\"url\",\" \",post)\n",
        "  post=re.sub(r\"\\s[^a-z]\\s\",\" \",post)\n",
        "  post=re.sub(r\"\\sop\\s\",\" \",post)\n",
        "  post=re.sub(r\"\\s[a-z]\\s\",\" \",post)  \n",
        "  post=re.sub(r\"\\s[a-z][a-z]\\s\",\" \",post)\n",
        "  post=re.sub(r\"\\w*\\d\\w*\",\"\",post)\n",
        "  post=re.sub(r\"\\.\",\" \",post)\n",
        "  post=re.sub(r\"\\\"\",\" \",post)\n",
        "  post=re.sub(r\"\\,\",\" \",post)\n",
        "  post=re.sub(r\"\\(|\\)|\\[|\\]|\\{|\\}\",\" \",post)\n",
        "  post=re.sub(r\"\\+|\\-|\\/|\\=|\\*\",\" \",post)\n",
        "  post.translate(str.maketrans('','',string.punctuation))\n",
        "  post=re.sub(r\"\\?\",\" \",post)\n",
        "  post=re.sub(r\"\\!\",\" \",post)\n",
        "  post=re.sub(r\"[^a-z]\",\" \",post)\n",
        "\n",
        "  return post\n",
        "depress_train[\"clean_text\"]=depress_train[\"clean_text\"].apply(lambda post:contractions(post))\n",
        "depress_test[\"clean_text\"]=depress_test[\"clean_text\"].apply(lambda post:contractions(post))\n",
        "depress_train[\"clean_text\"]=depress_train[\"clean_text\"].apply(lambda post:cleaning(post))\n",
        "depress_test[\"clean_text\"]=depress_test[\"clean_text\"].apply(lambda post:cleaning(post))\n",
        "depress_train[\"clean_text\"]=depress_train[\"clean_text\"].apply(lambda post:re.sub(\" +\",\" \",post))\n",
        "depress_test[\"clean_text\"]=depress_test[\"clean_text\"].apply(lambda post:re.sub(\" +\",\" \",post))\n",
        "anxiety_data[\"text\"]=anxiety_data[\"text\"].apply(lambda post:contractions(post))\n",
        "anxiety_data[\"text\"]=anxiety_data[\"text\"].apply(lambda post:cleaning(post))\n",
        "anxiety_data[\"text\"]=anxiety_data[\"text\"].apply(lambda post:re.sub(\" +\",\" \",post))\n",
        "stress_data[\"text\"]=stress_data[\"text\"].apply(lambda post:contractions(post))\n",
        "stress_data[\"text\"]=stress_data[\"text\"].apply(lambda post:cleaning(post))\n",
        "stress_data[\"text\"]=stress_data[\"text\"].apply(lambda post:re.sub(\" +\",\" \",post))\n",
        "anxiety_data.dropna(inplace=True)\n",
        "depress_train.dropna(inplace=True)\n",
        "depress_test.dropna(inplace=True)\n",
        "stress_data.dropna(inplace=True)\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "depress_train[\"clean_text\"]=[word_tokenize(w) for w in depress_train[\"clean_text\"]]\n",
        "depress_test[\"clean_text\"]=[word_tokenize(w) for w in depress_test[\"clean_text\"]]\n",
        "anxiety_data[\"text\"]=[word_tokenize(w) for w in anxiety_data[\"text\"]]\n",
        "stress_data[\"text\"]=[word_tokenize(w) for w in stress_data[\"text\"]]\n",
        "cleaned_words=[] \n",
        "#removing stopwords\n",
        "for words in depress_train[\"clean_text\"]:\n",
        "  temp=[]\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      temp.append(w)\n",
        "  cleaned_words.append(temp)\n",
        "depress_train[\"clean_text\"]=cleaned_words\n",
        "cleaned_words=[]\n",
        "for words in depress_test[\"clean_text\"]:\n",
        "  temp=[]\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      temp.append(w)\n",
        "  cleaned_words.append(temp)\n",
        "depress_test[\"clean_text\"]=cleaned_words\n",
        "cleaned_words=[]\n",
        "for words in anxiety_data[\"text\"]:\n",
        "  temp=[]\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      temp.append(w)\n",
        "  cleaned_words.append(temp)\n",
        "anxiety_data[\"text\"]=cleaned_words\n",
        "cleaned_words=[]\n",
        "for words in stress_data[\"text\"]:\n",
        "  temp=[]\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      temp.append(w)\n",
        "  cleaned_words.append(temp)\n",
        "stress_data[\"text\"]=cleaned_words      \n",
        "#lemmatzing by parts of speechh\n",
        "pos=[]\n",
        "for words in depress_train[\"clean_text\"]:\n",
        "  temp=nltk.pos_tag(words)\n",
        "  pos.append(temp)\n",
        "allowed_type=[\"JJ\",\"RB\",\"JJR\",\"JJS\",\"RBR\",\"RBS\",\"NN\",\"NNS\",\"NNP\"]\n",
        "convert={\"a\":[\"JJ\",\"JJR\",\"JJS\"],\"r\":[\"RB\",\"RBR\",\"RBS\"],\"n\":[\"NN\",\"NNS\",\"NNP\"]}\n",
        "cleaned_words=[]\n",
        "lematizer=WordNetLemmatizer();\n",
        "for words in pos:\n",
        "  temp=[]\n",
        "  for w in words:\n",
        "    if w[1] in allowed_type:\n",
        "      for key,val in convert.items():\n",
        "        if w[1] in val:\n",
        "          temp.append(lematizer.lemmatize(w[0],pos=key)) \n",
        "  cleaned_words.append(temp)\n",
        "depress_train[\"clean_text\"]=cleaned_words  \n",
        "\n",
        "pos=[]\n",
        "for words in depress_test[\"clean_text\"]:\n",
        "  temp=nltk.pos_tag(words)\n",
        "  pos.append(temp)\n",
        "cleaned_words=[]\n",
        "lematizer=WordNetLemmatizer();\n",
        "for words in pos:\n",
        "  temp=[]\n",
        "  for w in words:\n",
        "    if w[1] in allowed_type:\n",
        "      for key,val in convert.items():\n",
        "        if w[1] in val:\n",
        "          temp.append(lematizer.lemmatize(w[0],pos=key)) \n",
        "  cleaned_words.append(temp)\n",
        "depress_test[\"clean_text\"]=cleaned_words  \n",
        "pos=[]\n",
        "for words in anxiety_data[\"text\"]:\n",
        "  temp=nltk.pos_tag(words)\n",
        "  pos.append(temp)\n",
        "cleaned_words=[]\n",
        "lematizer=WordNetLemmatizer();\n",
        "for words in pos:\n",
        "  temp=[]\n",
        "  for w in words:\n",
        "    if w[1] in allowed_type:\n",
        "      for key,val in convert.items():\n",
        "        if w[1] in val:\n",
        "          temp.append(lematizer.lemmatize(w[0],pos=key)) \n",
        "  cleaned_words.append(temp)\n",
        "anxiety_data[\"text\"]=cleaned_words\n",
        "pos=[]\n",
        "for words in stress_data[\"text\"]:\n",
        "  temp=nltk.pos_tag(words)\n",
        "  pos.append(temp)\n",
        "cleaned_words=[]\n",
        "lematizer=WordNetLemmatizer();\n",
        "for words in pos:\n",
        "  temp=[]\n",
        "  for w in words:\n",
        "    if w[1] in allowed_type:\n",
        "      for key,val in convert.items():\n",
        "        if w[1] in val:\n",
        "          temp.append(lematizer.lemmatize(w[0],pos=key)) \n",
        "  cleaned_words.append(temp)\n",
        "stress_data[\"text\"]=cleaned_words \n",
        "depress_train[\"clean_text\"]=[list(set(t)) for t in depress_train[\"clean_text\"]]\n",
        "depress_test[\"clean_text\"]=[list(set(t)) for t in depress_test[\"clean_text\"]]\n",
        "stress_data[\"text\"]=[list(set(t)) for t in stress_data[\"text\"]]\n",
        "anxiety_data[\"text\"]=[list(set(t)) for t in anxiety_data[\"text\"]]\n",
        "\n",
        "anxiety_train=anxiety_data.sample(frac=0.8,random_state=50)\n",
        "anxiety_test=anxiety_data.drop(anxiety_train.index)\n",
        "stress_train=stress_data.sample(frac=0.8,random_state=50)\n",
        "stress_test=stress_data.drop(stress_train.index)\n",
        "\n",
        "dprs_word2vecmodel=Word2Vec(depress_train[\"clean_text\"].to_list(),size=100,window=3,min_count=2, workers=4,sg=1)\n",
        "anxty_word2vecmodel=Word2Vec(anxiety_train[\"text\"].to_list(),size=100,window=3,min_count=2, workers=4,sg=1)\n",
        "strs_word2vecmodel=Word2Vec(stress_train[\"text\"].to_list(),size=100,window=3,min_count=2, workers=4,sg=1)\n",
        "\n",
        "#vectorization\n",
        "def dprs_word2vectortest(tokenizedpost):\n",
        "  vectors=[]\n",
        "  if len(tokenizedpost)<1:\n",
        "    return np.zeros(100)\n",
        "  else:\n",
        "    for word in tokenizedpost:\n",
        "      if word in dprs_word2vecmodel.wv:\n",
        "        vectors.append(dprs_word2vecmodel.wv.get_vector(word))\n",
        "      else:\n",
        "        vectors.append(np.random.rand(100))\n",
        "  return np.mean(vectors,axis=0)          \n",
        "def anx_word2vectortest(tokenizedpost):\n",
        "  vectors=[]\n",
        "  if len(tokenizedpost)<1:\n",
        "    return np.zeros(100)\n",
        "  else:\n",
        "    for word in tokenizedpost:\n",
        "      if word in anxty_word2vecmodel.wv:\n",
        "        vectors.append(anxty_word2vecmodel.wv.get_vector(word))\n",
        "      else:\n",
        "        vectors.append(np.random.rand(100))\n",
        "  return np.mean(vectors,axis=0)\n",
        "def strs_word2vectortest(tokenizedpost):\n",
        "  vectors=[]\n",
        "  if len(tokenizedpost)<1:\n",
        "    return np.zeros(100)\n",
        "  else:\n",
        "    for word in tokenizedpost:\n",
        "      if word in strs_word2vecmodel.wv:\n",
        "        vectors.append(strs_word2vecmodel.wv.get_vector(word))\n",
        "      else:\n",
        "        vectors.append(np.random.rand(100))\n",
        "  return np.mean(vectors,axis=0)      \n",
        "depress_test.dropna(inplace=True)\n",
        "anxiety_test.dropna(inplace=True)\n",
        "depress_train.dropna(inplace=True)\n",
        "anxiety_train.dropna(inplace=True)\n",
        "stress_train.dropna(inplace=True)\n",
        "stress_test.dropna(inplace=True)\n",
        "depress_test=depress_test[depress_test.astype(str)['clean_text'] != '[]']\n",
        "anxiety_test=anxiety_test[anxiety_test.astype(str)['text'] != '[]']\n",
        "depress_train=depress_train[depress_train.astype(str)['clean_text'] != '[]']\n",
        "anxiety_train=anxiety_train[anxiety_train.astype(str)['text'] != '[]']\n",
        "stress_train=stress_train[stress_train.astype(str)['text'] != '[]']\n",
        "stress_test=stress_test[stress_test.astype(str)['text'] != '[]']\n",
        "depress_test[\"vectors\"]=depress_test[\"clean_text\"].apply(lambda post:dprs_word2vectortest(post))\n",
        "anxiety_test[\"vectors\"]=anxiety_test[\"text\"].apply(lambda post:anx_word2vectortest(post))\n",
        "stress_test[\"vectors\"]=stress_test[\"text\"].apply(lambda post:strs_word2vectortest(post))\n",
        "depress_train[\"vectors\"]=depress_train[\"clean_text\"].apply(lambda post:dprs_word2vectortest(post))\n",
        "anxiety_train[\"vectors\"]=anxiety_train[\"text\"].apply(lambda post:anx_word2vectortest(post))\n",
        "stress_train[\"vectors\"]=stress_train[\"text\"].apply(lambda post:strs_word2vectortest(post))\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics import accuracy_score      \n",
        "dep_rf=xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
        "y=np.array([np.array(post) for post in depress_train[\"vectors\"]])\n",
        "# print(y.shape)\n",
        "deprf_model=dep_rf.fit(y,depress_train[\"is_depression\"].values.ravel())\n",
        "y=np.array([np.array(post) for post in depress_test[\"vectors\"]])\n",
        "dep_pred=deprf_model.predict(y)\n",
        "# print(accuracy_score(depress_test[\"is_depression\"],dep_pred))\n",
        "def depress_rf(vector):\n",
        "  depress_pred=deprf_model.predict(vector)\n",
        "  return depress_pred\n",
        "anx_rf=xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
        "y=np.array([np.array(post) for post in anxiety_train[\"vectors\"]])\n",
        "anxrf_model=anx_rf.fit(y,anxiety_train[\"label\"].values.ravel())\n",
        "y=np.array([np.array(post) for post in anxiety_test[\"vectors\"]])\n",
        "anx_pred=anxrf_model.predict(y)\n",
        "# print(accuracy_score(anxiety_test[\"label\"],anx_pred))\n",
        "def anxiety_rf(vector):\n",
        "  anxiety_pred=anxrf_model.predict(vector)\n",
        "  return anxiety_pred\n",
        "strs_rf=xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
        "y=np.array([np.array(post) for post in stress_train[\"vectors\"]])\n",
        "strsrf_model=strs_rf.fit(y,stress_train[\"label\"].values.ravel())\n",
        "y=np.array([np.array(post) for post in stress_test[\"vectors\"]])\n",
        "strs_pred=strsrf_model.predict(y)\n",
        "# print(accuracy_score(stress_test[\"label\"],strs_pred))\n",
        "def stress_rf(vector):\n",
        "  stress_pred=strsrf_model.predict(vector)\n",
        "  return stress_pred\n",
        "def depressedsimilar(vector):\n",
        "  df1=depress_test[depress_test[\"is_depression\"]==1]\n",
        "  df2=depress_train[depress_train[\"is_depression\"]==1]\n",
        "  depressed=pd.concat([df1,df2])\n",
        "  test=np.array(vector)\n",
        "  templist=[]\n",
        "  for postvector in depressed[\"vectors\"]:\n",
        "    pvarray=np.array(postvector)\n",
        "    templist.append(np.dot(test,pvarray)/(norm(test)*norm(pvarray)))\n",
        "  depressed['similarity']=templist\n",
        "  depressed.sort_values(by=\"similarity\",ascending=False,inplace=True)\n",
        "  mostdepressed=depressed[\"similarity\"].iloc[0]\n",
        "  return mostdepressed\n",
        "def anxioussimilar(vector):\n",
        "  test=np.array(vector)\n",
        "  df1=anxiety_test[anxiety_test[\"label\"]==1]\n",
        "  df2=anxiety_train[anxiety_train[\"label\"]==1]\n",
        "  anxious=pd.concat([df1,df2])\n",
        "  templist=[]\n",
        "  for postvector in anxious[\"vectors\"]:\n",
        "    pvarray=np.array(postvector)\n",
        "    templist.append(np.dot(test,pvarray)/(norm(test)*norm(pvarray)))\n",
        "  anxious['similarity']=templist\n",
        "  anxious.sort_values(by=\"similarity\",ascending=False,inplace=True)\n",
        "  mostanxious=anxious[\"similarity\"].iloc[0]\n",
        "  return mostanxious\n",
        "def stresssimilar(vector):\n",
        "  df1=stress_test[stress_test[\"label\"]==1]\n",
        "  df2=stress_train[stress_train[\"label\"]==1]\n",
        "  stressed=pd.concat([df1,df2])\n",
        "  test=np.array(vector)\n",
        "  templist=[]\n",
        "  for postvector in stressed[\"vectors\"]:\n",
        "    pvarray=np.array(postvector)\n",
        "    templist.append(np.dot(test,pvarray)/(norm(test)*norm(pvarray)))\n",
        "  stressed['similarity']=templist\n",
        "  stressed.sort_values(by=\"similarity\",ascending=False,inplace=True)\n",
        "  moststressed=stressed[\"similarity\"].iloc[0]\n",
        "  return moststressed  \n",
        "def DASlabel(query):\n",
        "\n",
        "  text=re.sub(\"\\'\",\" \",query)\n",
        "  text=contractions(text)\n",
        "  text=cleaning(text)\n",
        "  post=re.sub(\" +\",\" \",text)\n",
        "  post=word_tokenize(post)\n",
        "  stop_words=set(stopwords.words(\"english\"))\n",
        "  temp=[]\n",
        "  for w in post:\n",
        "    if w not in stop_words:\n",
        "      temp.append(w)\n",
        "  post=temp\n",
        "  pos=[]\n",
        "  pos=nltk.pos_tag(post)\n",
        "  allowed_type=[\"JJ\",\"RB\",\"JJR\",\"JJS\",\"RBR\",\"RBS\",\"NN\",\"NNS\",\"NNP\"]\n",
        "  convert={\"a\":[\"JJ\",\"JJR\",\"JJS\"],\"r\":[\"RB\",\"RBR\",\"RBS\"],\"n\":[\"NN\",\"NNS\",\"NNP\"]}\n",
        "  cleaned_words=[]\n",
        "  lematizer=WordNetLemmatizer();\n",
        "  for w in pos:\n",
        "    if w[1] in allowed_type:\n",
        "      for key,val in convert.items():\n",
        "        if w[1] in val:\n",
        "          cleaned_words.append(lematizer.lemmatize(w[0],pos=key)) \n",
        "  post=list(set(cleaned_words))\n",
        "  dvectorizedpost=dprs_word2vectortest(post)\n",
        "  avectorizedpost=anx_word2vectortest(post)\n",
        "  svectorizedpost=strs_word2vectortest(post)\n",
        "  dv=np.array([np.array(dvectorizedpost)])\n",
        "  av=np.array([np.array(avectorizedpost)])\n",
        "  sv=np.array([np.array(svectorizedpost)])\n",
        "  depressed,anxious,stressed=depress_rf(dv),anxiety_rf(av),stress_rf(sv)\n",
        "  label=\" \"\n",
        "  if(int(depressed)>int(anxious) and int(depressed)>int(stressed)):\n",
        "    label=\"D\"\n",
        "  elif(int(anxious)>int(depressed) and int(anxious)>int(stressed)):\n",
        "    label=\"A\"\n",
        "  elif(int(stressed)>int(depressed) and int(stressed)>int(anxious)):\n",
        "    label=\"S\"\n",
        "  elif(int(anxious)==int(depressed) and int(depressed)==1 and int(stressed)==0):\n",
        "    dv=depressedsimilar(dvectorizedpost)\n",
        "    av=anxioussimilar(avectorizedpost)\n",
        "    if(dv.all()>av.all()):\n",
        "      label=\"D\"\n",
        "    else:\n",
        "      label=\"A\"\n",
        "  elif(int(anxious)==int(stressed) and int(stressed)==1 and int(depressed)==0):\n",
        "    sv=stresssimilar(svectorizedpost)\n",
        "    av=anxioussimilar(avectorizedpost)\n",
        "    if(sv.all()>av.all()):\n",
        "      label=\"S\"\n",
        "    else:\n",
        "      label=\"A\"    \n",
        "  elif(int(stressed)==int(depressed) and int(depressed)==1 and int(anxious)==0):\n",
        "    dv=depressedsimilar(dvectorizedpost)\n",
        "    sv=stresssimilar(svectorizedpost)\n",
        "    if(dv.all()>sv.all()):\n",
        "      label=\"D\"\n",
        "    else:\n",
        "      label=\"S\"\n",
        "  else:\n",
        "    dv=depressedsimilar(dvectorizedpost)\n",
        "    sv=stresssimilar(svectorizedpost)\n",
        "    av=anxioussimilar(avectorizedpost)\n",
        "    if(dv.all()>sv.all() and dv.all()>av.all()):\n",
        "      label=\"D\"\n",
        "    elif(sv.all()>dv.all() and sv.all()>av.all()):\n",
        "      label=\"S\"\n",
        "    else:\n",
        "      label=\"A\"\n",
        "  return label    \n",
        "\n",
        "# #precisioncalculation\n",
        "# def meanprecision(vector):\n",
        "#   result=pd.DataFrame()\n",
        "#   test=np.array(vector)\n",
        "#   temp=[]\n",
        "#   result['text']=depress_test[\"clean_text\"]\n",
        "#   result['vector']=depress_test[\"vectors\"]\n",
        "#   result['label']=depress_test[\"is_depression\"]\n",
        "#   for postvector in result[\"vector\"]:\n",
        "#     pvarray=np.array(postvector)\n",
        "#     if (norm(test)*norm(pvarray))==0:\n",
        "#       exit()\n",
        "#     else:\n",
        "#       temp.append(np.dot(test,pvarray)/(norm(test)*norm(pvarray)))\n",
        "#   result['similarity']=temp\n",
        "#   result.sort_values(by=\"similarity\",ascending=False,inplace=True)\n",
        "#   ranking=list(result[\"label\"].head(10))\n",
        "#   precision=[]\n",
        "#   for i in range(1,11):\n",
        "#     precision.append(np.sum(ranking[:i])/i)\n",
        "#   if precision==[]:\n",
        "#     return 0\n",
        "#   return np.mean(precision)\n",
        "# precision=[]\n",
        "# for i,post in depress_test.iterrows():\n",
        "#   if post[\"is_depression\"]==0:\n",
        "#     temp=meanprecision(post[\"vectors\"])\n",
        "#     precision.append(1-temp)\n",
        "#   else:\n",
        "#     precision.append(meanprecision(post[\"vectors\"]))  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # print(\"precision:\",np.mean(precision))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "def videototext(videopath):\n",
        "  model1 = whisper.load_model(\"base\")\n",
        "  text = model1.transcribe(videopath,fp16=False,language=\"en\")\n",
        "  return text['text']\n"
      ],
      "metadata": {
        "id": "qIZ2R1UI0UP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import werkzeug\n",
        "from flask import Flask,request,jsonify\n",
        "import numpy as np\n",
        "import nltk\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app=Flask(__name__)\n",
        "port=5000\n",
        "ngrok.set_auth_token(\"2LkQXIb6v3NfSP6gOta9IbhZHqt_819dkaTGBDo5EZ5ctxZ1Z\")\n",
        "public_url=ngrok.connect(port).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> n http://127.0.0.1:{}\\\"\".format(public_url,port))\n",
        "app.config[\"BASE_URL\"]=public_url\n",
        "@app.route('/api',methods=['GET'])\n",
        "def DAS_text():\n",
        "    print(\"in DAS function\")\n",
        "    query=str(request.args['text'])\n",
        "    label=DASlabel(query)\n",
        "    d={}\n",
        "    d['label']=label    \n",
        "    return jsonify(d)\n",
        "\n",
        "@app.route('/video',methods=['POST'])\n",
        "def convertvideo():\n",
        "  if(request.method==\"POST\"):\n",
        "      \n",
        "    videofile = request.files[\"video\"]\n",
        "    print(\"after videofile\")\n",
        "    filename = werkzeug.utils.secure_filename(videofile.filename)\n",
        "        # print(\"\\nReceived video File name : \" + videofile.filename)\n",
        "    videofile.save(filename)\n",
        "    \n",
        "    print(\"file saved!!!\")\n",
        "    text=videototext(filename)\n",
        "    label=DASlabel(text)\n",
        "    d={}\n",
        "    d['label']=label\n",
        "    response=jsonify(d)\n",
        "    response.headers.add(\"Access-Control-Allow-Origin\", \"*\")    \n",
        "\n",
        "    return response\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyGvM5cH81NU",
        "outputId": "57bb895b-c234-42fb-c5c8-376bd67fbac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * ngrok tunnel \"http://b646-35-194-78-113.ngrok.io\" -> n http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}